{"cells":[{"cell_type":"markdown","source":["# Credit Card Fraud\n* Resources\n  * From https://datascience.ibm.com/exchange/public/entry/view/d80de77f784fed7915c14353512ef14d"],"metadata":{}},{"cell_type":"markdown","source":["## Individual Project in Spark\n\n**By:**\n\n- Ashish Devrani\n- Panther# (002329273)"],"metadata":{}},{"cell_type":"markdown","source":["## Objective\n* The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\n* It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n* Use Logistic Regression\n  * Train on a portion of the dataset\n  * Test the trained model against the remainder of the dataset\n    * Accuracy can be determined because the dataset is labeled (i.e., this uses supervised learning)\n\n* Result:\n  * In this kind of problem, instead of acheving an oveall accuracy, we would like to know how many fraud transactions were correctly identified and how many were missed.\n  * Using Logistic Regression for prediction of fraud trannsaction, we were able to acheve a 63% accuracy in identifying the number of frauds."],"metadata":{}},{"cell_type":"markdown","source":["## Download Data"],"metadata":{}},{"cell_type":"code","source":["# Using the below command we are importing our dataset, which is a csv file from our shared folder on Onedrive"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sh\nwget --no-check-certificate 'https://onedrive.live.com/download?cid=77DD390E20E2AD35&resid=77DD390E20E2AD35%212201&authkey=AEz_A1GDynO8vq0' -O CreditCardData.csv\nls /databricks/driver"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# listing all the files at the location /databrics/driver to check if our file CreditCardData.csv was saved"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sh\nls"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Using the below command we are trying to read the imported file as a dataframe into the Spark memory"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["creditcard_df = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .option('inferSchema', 'true')\\\n  .load(\"file:/databricks/driver/CreditCardData.csv\")\ncreditcard_df.show(10)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Data Explored and Explained"],"metadata":{}},{"cell_type":"code","source":["creditcard_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["creditcard_df.count()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Data Cleaning"],"metadata":{}},{"cell_type":"code","source":["# Here we have tried to clean our data of null values.\n#Observation: The data that we got was really clean and there was no null column. We measured this by calculating the number of data rows before and after the cleaning operation for null values."],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["cleanedcc_df = creditcard_df.filter(creditcard_df.Amount.isNotNull() & creditcard_df.V1.isNotNull() & creditcard_df.V2.isNotNull()& creditcard_df.V3.isNotNull()& creditcard_df.V4.isNotNull()& creditcard_df.V5.isNotNull()& creditcard_df.V6.isNotNull()& creditcard_df.V7.isNotNull()& creditcard_df.V8.isNotNull()& creditcard_df.V9.isNotNull()& creditcard_df.V10.isNotNull()& creditcard_df.V11.isNotNull()& creditcard_df.V12.isNotNull()& creditcard_df.V13.isNotNull()& creditcard_df.V14.isNotNull()& creditcard_df.V15.isNotNull()& creditcard_df.V16.isNotNull()& creditcard_df.V17.isNotNull()& creditcard_df.V18.isNotNull()& creditcard_df.V19.isNotNull()& creditcard_df.V20.isNotNull()& creditcard_df.V21.isNotNull()& creditcard_df.V22.isNotNull()& creditcard_df.V23.isNotNull()& creditcard_df.V24.isNotNull()& creditcard_df.V25.isNotNull()& creditcard_df.V26.isNotNull()& creditcard_df.V27.isNotNull()& creditcard_df.V28.isNotNull()& creditcard_df.Time.isNotNull())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["creditcard_df.count()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Here we are trying to classify the data, check if any values apart from 0 and one exist in our predictor column, then such values need to be cleaned or normalized."],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# The transaction to be predicted - Good transaction or Fraud transaction\ncleanedcc_df.select('Class').distinct().show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# transactions by their targets - 1 is the number of fraud transactions while 0 is the number of good transactions.\ncleanedcc_df.select('Class').groupBy('Class').count().show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## Data Transformation"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import *\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Convert results for to MLlib input, which requires labels as a float\ndef labelForResults(s):\n     if s == 0:\n         return 0.0\n     elif s == 1:\n         return 1.0\n     else:\n         return -1.0\nlabel = UserDefinedFunction(labelForResults, DoubleType())\n\nlabeledData = cleanedcc_df.select(label(cleanedcc_df.Class).alias('label'),cleanedcc_df.V1, cleanedcc_df.V2, cleanedcc_df.V3, cleanedcc_df.V4, cleanedcc_df.V5, cleanedcc_df.V6, cleanedcc_df.V7, cleanedcc_df.V8, cleanedcc_df.V9, cleanedcc_df.V10, cleanedcc_df.V11, cleanedcc_df.V12, cleanedcc_df.V13, cleanedcc_df.V14, cleanedcc_df.V15, cleanedcc_df.V16, cleanedcc_df.V17, cleanedcc_df.V18, cleanedcc_df.V19, cleanedcc_df.V20, cleanedcc_df.V21, cleanedcc_df.V22, cleanedcc_df.V23, cleanedcc_df.V24, cleanedcc_df.V25, cleanedcc_df.V26, cleanedcc_df.V27, cleanedcc_df.V28, cleanedcc_df.Time, cleanedcc_df.Amount).where('label >= 0')\nlabeledData.show(10)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Split into training and testing data\ncreditcard_train, creditcard_test = labeledData.randomSplit([0.8, 0.2], seed=12345)\ndisplay(creditcard_train)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["## Data Modeling"],"metadata":{}},{"cell_type":"code","source":["# Configure an ML pipeline into stages:\nvectorAssembler_features = VectorAssembler(inputCols = [x for x in labeledData.columns if x not in \"label\"], outputCol=\"features\")\nlr = LogisticRegression(maxIter=15, regParam=0.0005)\npipeline = Pipeline(stages=[vectorAssembler_features, lr])"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# In this step we are pushing our training data into the ML pipeline, that we have created above"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["model = pipeline.fit(creditcard_train)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["## Prediction"],"metadata":{}},{"cell_type":"code","source":["predictionsDf = model.transform(creditcard_test)\npredictionsDf.registerTempTable('Predictions')\npredictionsDf.show(3)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["## Model Evaluation"],"metadata":{}},{"cell_type":"code","source":["numSuccesses = predictionsDf.where(\"(label = 1 AND prediction = 1)\").count()\nnumFailure= predictionsDf.where(\"(label = 1 AND prediction = 0)\").count()\nnumData = predictionsDf.count()\n\nprint \"There were\", numSuccesses+numFailure , \"fraud transactions in a total pool of\", numData , \"available transactions for prediction, out of which -\", numSuccesses, \" fraud transactions were successfully predicted and \",numFailure,\"fraud transactions were missed out or unsuccessfully predicted as good transactions.\"\nprint \"This is a\", str((float(numSuccesses) / float(numSuccesses+numFailure)) * 100) + \"%\", \"success rate in identifying the fraud transactions.\""],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# Using the graphical reprasentation below, I have shown how many fraud transactions were correctly predicted and how many fraud transactions were predicted as good transaction. Here I have achieved a 63% accuracy in a transaction database where fraud transactions were just 0.172% of the total data. Hence we found the needle in a haystack."],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["truePositive = int(predictionsDf.where(\"(label = 1 AND prediction = 1)\").count())\ntrueNegative = int(predictionsDf.where(\"(label = 0 AND prediction = 0)\").count())\nfalsePositive = int(predictionsDf.where(\"(label = 0 AND prediction = 1)\").count())\nfalseNegative = int(predictionsDf.where(\"(label = 1 AND prediction = 0)\").count())\n\nresultTDF = sqlContext.createDataFrame([['TP', truePositive], ['FN', falseNegative] ], ['metric', 'value'])\ndisplay(resultTDF)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["display(resultTDF)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# The number of good transactions predicted is high as expected as the good transaction data was already 99.87 percent and even if we would have predicted all labels as 0, we would have achieved high accuracy in this domain."],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["resultNDF = sqlContext.createDataFrame([['TN', trueNegative], ['FP', falsePositive] ], ['metric', 'value'])\ndisplay(resultNDF)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["display(resultNDF)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# --------------------- Visualisations using R ------------------------------"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["resultTDF.createOrReplaceTempView(\"Tresult\")"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["%r\nlibrary(SparkR)\nsparkdf <- sql(\"FROM Tresult SELECT *\")\ntdf <- collect(sparkdf)\nprint( tdf)\nvals <- (t(tdf[2]))\nlabels <- (t(tdf[1]))\n# Simple Pie Chart\npie(vals,labels)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["resultNDF.createOrReplaceTempView(\"Fresult\")"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["%r\nlibrary(SparkR)\nsparkdf2 <- sql(\"FROM Fresult SELECT *\")\nfdf <- collect(sparkdf2)\nprint( fdf)\nvals <- (t(fdf[2]))\nlabels <- (t(fdf[1]))\n# Simple Pie Chart\npie(vals,labels)"],"metadata":{},"outputs":[],"execution_count":43}],"metadata":{"name":"Credit Card Prediction","notebookId":2891587701121673},"nbformat":4,"nbformat_minor":0}
